cran
?select
select(cran, ip_id, package, country)
5:20
select(cran, r_arch:country)
select(cran, country: r_arch)
head(cran)
cran
cran
select(cran, -time)
select(-5:20)
select(cran, -(X:size))
-5:20
-(5:20)
select(cran, -(X:size))
cran
filter(cran, package == "swirl")
filter(cran, r_version =="3.1.1", country =="US")
?Comparison
filter(cran, r_version =="3.0.2", country =="IN")
filter(cran, r_version <="3.0.2", country =="IN")
filter(cran, country =="US"| country = "IN")
filter(cran, country =="US"| country == "IN")
filter(cran, size > 100500 & r_os =="linux-gnu")
filter(cran, size > 100500, r_os =="linux-gnu")
is.na(c(3,5,NA,10))
!is.na(c(3,5,NA,10))
filter(cran, !is.na(r_version))
cran2 <- select(cran, size:ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id)
)
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb = size/ 2^20)
mutate(cran3, size_gb = size_mb/ 2^10)
mutate(cran3, size_mb = size/ 2^20, size_gb = size_mb/ 2^10)
mutate(cran3, correct_size = size/1000)
mutate(cran3, correct_size = size +1000)
summarize(cran, avg_bytes = mean(size))
quit()
##library(httr)
##library(jsonlite)
myapp = oauth_app("twitter",
key="HCAu0l3vJk2VVIslCgi2R0ayZ",secret="mf7GB62x6qcndJrdKpYMMbA34eosDerEdicrO3S5pFJ9JXGLVQ")
sig = sign_oauth1.0(myapp,
token = "2836401709-P3HvwjQqrOzialfOabYpS4vdBirZHnhCOnDDpSo",
token_secret = "eoBbslfkId2HL21DTtLpiKb3l7wkAXZm72TuHJksufMgF")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1:5,4]
library(httr)
library(jsonlite)
myapp = oauth_app("twitter",
key="HCAu0l3vJk2VVIslCgi2R0ayZ",secret="mf7GB62x6qcndJrdKpYMMbA34eosDerEdicrO3S5pFJ9JXGLVQ")
sig = sign_oauth1.0(myapp,
token = "2836401709-P3HvwjQqrOzialfOabYpS4vdBirZHnhCOnDDpSo",
token_secret = "eoBbslfkId2HL21DTtLpiKb3l7wkAXZm72TuHJksufMgF")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1:5,4]
quit()
x <- c("a", "b", "c", "d")
for (i in x) {}
x <- c("a", "b", "c", "d")
for (i in x) {
print(x)
}
x <- c("a", "b", "c", "d")
for (i in 1:4) {
print(x)
}
x <- c("a", "b", "c", "d")
for (i in 1:4) {
print(x)
}
x <- c("a", "b", "c", "d")
for (i in 1:4) {
print(x[i])
}
x <- c("a", "b", "c", "d")
for (i in 1:4) {
print(x[i])
}
x <- c("a", "b", "c", "d")
for (i in 1:4) {
print(x[i])
}
x <- c("a", "b", "c", "d")
for (i in 1:4) {
print(x[i])
}
z <- 5
while(z> 3 && z<=10){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 5
while(z> 1 && z<=10){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 5
while(z> 1 && z<=10){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 10
while(z> 1 && z<=20){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 10
while(z> 1 && z<=20){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 20
while(z> 1 && z<=40){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 20
while(z> 1 && z<=40){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 20
while(z> 1 && z<=40){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 20
while(z> 1 && z<=40){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 20
while(z> 1 && z<=40){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 50
while(z> 1 && z<=100){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 500
while(z> 1 && z<=1000){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 500
while(z> 1 && z<=1000){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 500
while(z> 1 && z<=1000){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
z <- 500
while(z> 1 && z<=1000){
print(z)
coin <- rbinom(1,1,0.5)
if(coin==1){
z <- z+1
}else{
z <- z-1
}
}
quit()
cube <- function(x, n) {
x^3
}
cube(3)
x <- 1:10
if(x > 5) {
x <- 0
}
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
z <- 10
f(3)
y
x <- 5
y <- if(x < 3) {
NA
} else {
10
}
y
swirl()
library("swirl")
rm(list=ls())
swirl()
x
x[1:10]
x[is.na(x)]
y <- x[!is.na(x)]
y
y[y>0]
x[x>0]
x[!is.na(x) & x >0]
x[c(3,5,7)]
x[0]
x[3000]
x[c(-2,-10)]
x[-c(2,10)]
vect <- c(foo=11, bar =2, norf= NA)
vect
names(vect)
vect2 <- c(11, 2, NA)
names(vect2) <- c("foo", "bar", "norf")
identical(vect, vect2)
vect["bar"]
vect[c("foo", "bar")]
swirl()
head(flags)
dim(flags)
class(flags)
cls_list <- lapply(flags, class)
cls_list
class(cls_list)
as.character(cls_list)
?sapply
cls_vect <- sapply(flags, class)
class(cls_vect)
sum(flags@orange)
sum(flags$orange)
flags_colors <- flags[,11:17]
flag_colors <- flags[,11:17]
head(flag_colors)
lapply(flags_colors, sum)
lapply(flag_colors, sum)
sapply(flag_colors, sum)
sapply(flag_colors, mean)
flag_shapes <- flags[,19:23]
lapply(flag_shapes, range)
sapply(flag_shapes, range)
shape_mat <- sapply(flag_shapes, range)
shape_mat
class(shape_mat)
unique(c(3, 4, 5, 5, 5, 6, 6))
unique_vals <- lapply(flags, unique)
unique_vals
sapply(unique_vals, length)
sapply(flags, unique)
lapply(unique_vals, function(elem) elem[2])
sapply(flags, unique)
vapply(flags, unique, numeric(1))
ok()
sapply(flags, class)
vaplly(flags, class, character(1))
vapply(flags, class, character(1))
?tapply
table(flags$landmass)
table(flags$animate)
tapply(flags$animate, flags$landmass, mean)
tapply(flags$population, flags$red, summary)
tapply(flags$population, flags$landmass, summary)
ls()
class(plants)
dim(plants)
nrow(plants)
ncol(plants)
object.size(plants)
names(plants)
head(plants)
head(plants, nrow=10)
head(plants, 10)
tail(plants, 15)
summary(plants)
table(plants$Active_Growth_Period)
str(plants)
library(httr)
library(jsonlite)
myapp = oauth_app("twitter",
key="HCAu0l3vJk2VVIslCgi2R0ayZ",secret="mf7GB62x6qcndJrdKpYMMbA34eosDerEdicrO3S5pFJ9JXGLVQ")
sig = sign_oauth1.0(myapp,
token = "2836401709-P3HvwjQqrOzialfOabYpS4vdBirZHnhCOnDDpSo",
token_secret = "eoBbslfkId2HL21DTtLpiKb3l7wkAXZm72TuHJksufMgF")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1:5,4]
summary(DT2)
quit()
xyploy(Ozone ~ Wind, data=airquality)
xyplot(Ozone ~ Wind, data=airquality)
install.packages(lattice)
install.packages("lattice")
library(lattice)
xyplot(Ozone ~ Wind, data=airquality)
install.packages("ggplot2")
library(ggplot2)
qplot(displ, hwy, data ="mpg")
qplot(displ, hwy, data =mpg)
qplot(displ, hwy, data =mpg, color = drv)
qplot(displ, hwy, data =mpg, geom = c("point", "smooth")
)
qplot(displ, hwy, data=mpg, facet=.~drv)
qplot(displ, hwy, data=mpg, facets=.~drv)
summary(maacs)
library(ggplot2)
#read in the data
NEI <- readRDS("summarySCC_PM25.rds")
SCC <- readRDS("Source_Classification_Code.rds")
#merge the 2 tables
NEISCC <- merge(NEI, SCC, by = "SCC")
#get the motor vehicle related data
vehicle <- NEISCC[grep("Vehicle", NEISCC$SCC.Level.Two),]
#subset the baltimore data
#LAbalt <- vehicle[grep("24510" | "06037", vehicle$fips),]
#LAbalt <- subset(vehicle, vehicle$fips=="24510" & "06037")
LAbalt <- subset(vehicle, vehicle$fips=="24510" | vehicle$fips== "06037")
#LA <- subset(vehicle, vehicle$fips=="06037")
#aggregate the data
total <- aggregate(Emissions ~ year+fips, LAbalt, sum)
#create plot
g <- ggplot(total, aes(year, Emissions))
g + geom_point(alpha = 2/3)+
ggtitle("Emissions from Motor Vehicle sources(Los Angeles v Baltimore)")+
geom_smooth(col="steelblue")+
facet_grid( ~ fips,)+
scale_y_continuous(formatter = 'percent')
#create png
dev.copy(png, file="plot6alt.png", width=480, height=480)
dev.off()
source('~/Desktop/coursera/EDA/project2/plot6alt.R')
library(httr)
library(jsonlite)
myapp = oauth_app("twitter",
key="HCAu0l3vJk2VVIslCgi2R0ayZ",secret="mf7GB62x6qcndJrdKpYMMbA34eosDerEdicrO3S5pFJ9JXGLVQ")
sig = sign_oauth1.0(myapp,
token = "2836401709-P3HvwjQqrOzialfOabYpS4vdBirZHnhCOnDDpSo",
token_secret = "eoBbslfkId2HL21DTtLpiKb3l7wkAXZm72TuHJksufMgF")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1:5,4]
library(httr)
library(jsonlite)
myapp = oauth_app("twitter",
key="HCAu0l3vJk2VVIslCgi2R0ayZ",secret="mf7GB62x6qcndJrdKpYMMbA34eosDerEdicrO3S5pFJ9JXGLVQ")
sig = sign_oauth1.0(myapp,
token = "2836401709-P3HvwjQqrOzialfOabYpS4vdBirZHnhCOnDDpSo",
token_secret = "eoBbslfkId2HL21DTtLpiKb3l7wkAXZm72TuHJksufMgF")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1:5,4]
quit()
quit()
quit()
setwd("~/Desktop/coursera/RepRes/RepData_PeerAssessment1")
---
title: "Reproducible Research: Peer Assessment 1"
output:
html_document:
keep_md: true
---
## Loading and preprocessing the data
Load the data (i.e. read.csv())
```{r}
library(knitr)
actraw <- read.csv("activity.csv", header = T, na.strings="NA")
```
Process/transform the data (if necessary) into a format suitable for your analysis
```{r}
act <- na.omit(actraw)
aggdate <- aggregate(steps ~ date, act, sum)
```
## What is mean total number of steps taken per day?
Make a histogram of the total number of steps taken each day
```{r}
hist(aggdate$steps, main= "Histogram of total steps per day frequency", col= "red", xlab= "total steps")
```
Calculate and report the mean and median total number of steps taken per day
```{r}
mean(aggdate$steps)
median(aggdate$steps)
```
## What is the average daily activity pattern?
Make a time series plot (i.e. type = "l") of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all days (y-axis)
```{r}
byinterval <- aggregate(steps ~ interval, act, mean)
colnames(byinterval) <- c("interval", "stepsavg")
plot(byinterval$interval, byinterval$stepsavg, type="l", main="Average number of steps taken in each 5 min interval", col="green")
```
Which 5-minute interval, on average across all the days in the dataset, contains the maximum number of steps?
```{r}
max <- max(byinterval$stepsavg)
maxint <- subset(byinterval, stepsavg == max)
print(maxint)
```
## Imputing missing values
Calculate and report the total number of missing values in the dataset (i.e. the total number of rows with NAs)
```{r}
sum(is.na(actraw$steps))
```
Create a new dataset that is equal to the original dataset but with the missing data filled in.
```{r}
newact <- merge(actraw, byinterval, by ="interval")
newact$steps <- with(newact, ifelse(is.na(steps), stepsavg,steps))
nafill <- aggregate(steps ~ date, newact, sum)
```
Make a histogram of the total number of steps taken each day and Calculate and report the mean and median total number of steps taken per day. Do these values differ from the estimates from the first part of the assignment? What is the impact of imputing missing data on the estimates of the total daily number of steps?
```{r}
hist(nafill$steps, main= "Histogram of total steps p/day(NA's filled)", col= "orange", xlab= "total steps")
```
```{r}
mean(nafill$steps)
median(nafill$steps)
```
## Are there differences in activity patterns between weekdays and weekends?
Create a new factor variable in the dataset with two levels -- "weekday" and "weekend" indicating whether a given date is a weekday or weekend day.
```{r}
newact$day <- weekdays(as.Date(newact$date))
newact$wkd <- as.factor(ifelse(newact$day %in% c("Saturday","Sunday"), "Weekend", "Weekday"))
nonaint <- aggregate(steps~ interval+wkd, newact, mean)
```
Make a panel plot containing a time series plot (i.e. type = "l") of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all weekday days or weekend days (y-axis).
```{r}
library(lattice)
xyplot(steps ~ interval| wkd, data = nonaint,
type = "l", xlab = "Interval",ylab = "Number of steps",
layout=c(1,2))
#knit2html("PA1_template.Rmd")
```
knit2html("PA1_template.Rmd")
quit()
